<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>无标题文档</title>
<style type="text/css">
.color {
	font-family: Verdana, Geneva, sans-serif;
	font-size: 16px;
	background-color: #F9F9F9;
	color: #F00;
	font-weight: bold;
}
.mCenter {
	text-align: center;
	font-weight: bold;
}
.jiacu {
	font-weight: bold;
}
</style>
</head>

<body>
<h1 class="mCenter"><strong>We thank the four reviewers 045A, 284F, 4BF9, and 0F1D for their review and valuable suggestions, and we have revised the text accordingly, and we are deeply grateful for your guidance.
</strong></h1>
<p><strong class="color">Response from Review 045A:</strong><br />
1.Our methods SPGFusion and SeAFusion are completely different approaches to image fusion.SeAFusion is the current SOAT method, and its fusion process consists of two key components: a fusion network and a semantic segmentation network.SeAFusion constrains the fusion network to include more semantic information by jointly training the two networks. In contrast, our method SPGFusion utilizes semantic prior knowledge by making the infrared or visible map feature maps contain more semantic information in the feature extraction phase and combining semantic adversarial generation loss to obtain better quality fusion results.</p>
<p>2. In terms of performance, we have conducted a detailed comparison of our method with nine current leading methods, as shown in the paper, and our method performs well on the MSRS dataset, significantly outperforming the other nine methods, including even SeAFusion, which is widely regarded as the most superior method in the field of fusion, as mentioned by the reviewer. In order to fully compare the performance of our method with SeAFusion, we performed a generalised analysis comparison and selected the M3FD dataset. On this basis, we randomly selected 20 pairs of fusion results for mean calculation. The results show that our method also exhibits excellent performance on the M3FD dataset: it outperforms SeAFusion in terms of mutual information (MI), visual information fidelity (VIF), average gradient (AG), spatial frequency (SF), and quality of fusion (Qabf), which suggests that the fused images generated by our method are more similar to the original infrared images and visible images, with better image fidelity, presents richer details and variations, as well as obtains better fused image quality.<br />
</p>
<p class="mCenter"><span class="mCenter"><span class="mCenter"><span class="mCenter">Table 1:Comparative generalisation analysis of </span></span></span>SPGFusion<span class="mCenter"><span class="mCenter"><span class="mCenter"> with SeAFusion on M3FD dataset</span></span></span></p>
<div align="center">
  <table border="1" cellspacing="0" cellpadding="0" width="442">
    <tr>
      <td width="82" nowrap="nowrap"><strong> Method </strong></td>
      <td width="72" nowrap="nowrap"><p align="center"><strong>MI</strong></p></td>
      <td width="72" nowrap="nowrap"><p align="center"><strong>VIF</strong></p></td>
      <td width="72" nowrap="nowrap"><p align="center"><strong>AG</strong></p></td>
      <td width="72" nowrap="nowrap"><p align="center"><strong>Qabf</strong></p></td>
      <td width="72" nowrap="nowrap"><p align="center"><strong>SF</strong></p></td>
    </tr>
    <tr>
      <td width="82" nowrap="nowrap" valign="bottom"><p align="center">SeAFusion</p></td>
      <td width="72" nowrap="nowrap"><p align="center">2.8996</p></td>
      <td width="72" nowrap="nowrap"><p align="center">0.8261</p></td>
      <td width="72" nowrap="nowrap"><p align="center">4.4289</p></td>
      <td width="72" nowrap="nowrap"><p align="center">0.6711</p></td>
      <td width="72" nowrap="nowrap"><p align="center">13.7051</p></td>
    </tr>
    <tr>
      <td width="82" nowrap="nowrap" valign="bottom"><p align="center">Our</p></td>
      <td width="72" nowrap="nowrap"><p align="center">3.6612</p></td>
      <td width="72" nowrap="nowrap"><p align="center">0.864</p></td>
      <td width="72" nowrap="nowrap"><p align="center">4.5295</p></td>
      <td width="72" nowrap="nowrap"><p align="center">0.7044</p></td>
      <td width="72" nowrap="nowrap"><p align="center">13.8751</p></td>
    </tr>
  </table>
</div>
<p><br />
</p>
<p>3. In order to verify the effect of adversarial loss and content loss separately, the results after removing only adversarial loss and only content loss are used as shown below: <br />
</p>
<p>Since retraining the network takes longer, we will mention this process in the article but will not show it here.</p>
<p>&nbsp;</p>
<p><span class="color"><strong class="color">Response from Review</strong> 284F:</span></p>
<p align="left">1. How stable is the training of WGAN, and could the  authors elaborate on the consideration of the Lipschitz constant in the loss  functions of WGAN? The absence of clarification on this constraint leaves room  for uncertainty and warrants further explanation.<br />
The addition of the gradient penalty term has two main effects: </p>
<p>(i) it makes the penalty smaller for larger IR or visible gradient values, and larger for smaller gradient values, thus prompting the fused image to generate more edge and gradient information. </p>
<p>(ii) make the model input has a small change, the network weights will not produce too much change, as far as possible to reduce the generation of mode collapse problem.</p>
<p>&nbsp;</p>
<p>2. What is the difference between semantic feature awareness module between Wu et al.[17]?<br />
</p>
<p> The proposed  method and the method of Wu et al [17] have the following different operations  in the semantic perception module: in the Pre-Processing stage, for visible or  infrared feature maps with large channels, we reduce the number of feature map  channels by using a 1x1 convolutional kernel, and subsequently compute the  cost-attention maps, which reduces the GPU memory footprint.<br />
In the FeedForward  stage, we fuse the IR or visible feature maps with the semantic feature maps by  element-by-element multiplication and introduce the nonlinear relation GELU  function. Different from Wu et al.'s approach [17], they generate two copies of  semantic-aware feature maps by convolutional operation and then multiply these  two copies together. Our method enhances the semantic fusion capability and  introduces nonlinear elements to improve the expressive power of the model.</p>
<p><br />
  <br />
</p>
<p><strong class="color"><span class="color">Response from Review</span></strong><span class="color"> 4BF9:</span></p>
<p>1.Please compare the model complexity and the run time complexity of the proposed model with those of some selected SOTAs.</p>
<p class="mCenter">Table 2: SPGFusion efficiency analysis, which compares it to each of the nine current state-of-the-art methods.</p>
<div align="center">
  <table width="920" border="1" align="center" cellpadding="0" cellspacing="0">
    <tr>
      <td width="73" nowrap="nowrap"></td>
      <td width="89" nowrap="nowrap"><p align="center"><strong>FusionGan</strong></p></td>
      <td width="95" nowrap="nowrap"><p align="center"><strong>DenseFuse</strong></p></td>
      <td width="85" nowrap="nowrap"><p align="center"><strong>DIDFuse</strong></p></td>
      <td width="66" nowrap="nowrap"><p align="center"><strong>PMGI</strong></p></td>
      <td width="80" nowrap="nowrap"><p align="center"><strong>ICAFusion</strong></p></td>
      <td width="80" nowrap="nowrap"><p align="center"><strong>IFCNN</strong></p></td>
      <td width="76" nowrap="nowrap"><p align="center"><strong>GANMcc</strong></p></td>
      <td width="92" nowrap="nowrap"><p align="center"><strong>SeAFusion</strong></p></td>
      <td width="73" nowrap="nowrap"><p align="center"><strong>U2Fusion</strong></p></td>
      <td width="111" nowrap="nowrap"><p align="center"><strong>Our</strong></p></td>
    </tr>
    <tr>
      <td width="73" nowrap="nowrap"><p align="center">Size(MB)</p></td>
      <td width="89" nowrap="nowrap"><p align="center">0.074</p></td>
      <td width="95" nowrap="nowrap"><p align="center">0.075</p></td>
      <td width="85" nowrap="nowrap"><p align="center">0.261</p></td>
      <td width="66" nowrap="nowrap"><p align="center">0.042</p></td>
      <td width="80" nowrap="nowrap"><p align="center">2.471</p></td>
      <td width="80" nowrap="nowrap"><p align="center">0.084</p></td>
      <td width="76" nowrap="nowrap"><p align="center">1.864</p></td>
      <td width="92" nowrap="nowrap"><p align="center">0.167</p></td>
      <td width="73" nowrap="nowrap"><p align="center">0.659</p></td>
      <td width="111" nowrap="nowrap"><p align="center">3.727</p></td>
    </tr>
    <tr>
      <td width="73" nowrap="nowrap"><p align="center">Time(s)</p></td>
      <td width="89" nowrap="nowrap"><p align="center">0.122</p></td>
      <td width="95" nowrap="nowrap"><p align="center">3.435</p></td>
      <td width="85" nowrap="nowrap"><p align="center">0.053</p></td>
      <td width="66" nowrap="nowrap"><p align="center">0.058</p></td>
      <td width="80" nowrap="nowrap"><p align="center">9.398</p></td>
      <td width="80" nowrap="nowrap"><p align="center">0.021</p></td>
      <td width="76" nowrap="nowrap"><p align="center">0.338</p></td>
      <td width="92" nowrap="nowrap"><p align="center">0.655</p></td>
      <td width="73" nowrap="nowrap"><p align="center">0.057</p></td>
      <td width="111" nowrap="nowrap"><p align="center">4.708</p></td>
    </tr>
  </table>
</div>
<p>We analysed model size (Mb) and inference time (s) for all compared methods, including several state-of-the-art (SOTA) methods. Note that each model was performed on a single NVIDIA 3060Ti GPU, with the input scaled to 640 × 480. from the results in Table 1, our SPGFusion ranks highest in terms of model size, with a maximum parameter count of 3.727 MB. Meanwhile, the inference time ranks second to the bottom, a phenomenon attributed to our adopted a semantic feature-aware approach. Therefore, how to achieve a better balance between performance and efficiency has become an important direction for our future research.<br />
</p>
<p>&nbsp;</p>
<p><strong class="color"><span class="color">Response from Review</span></strong><span class="color"> 0F1D :</span></p>
<p><br />
1) Fig1 and Fig2 are not clear enough, you can try vector diagram.  2) You should cite other method in Table1.</p>
<p>We will use vector graphics in the paper for modification to improve the clarity of the images. In the meantime, we have added references to other methods in Table1.<br />
</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
</body>
</html>
