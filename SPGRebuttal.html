<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>无标题文档</title>
<style type="text/css">
.color {
	font-family: Verdana, Geneva, sans-serif;
	font-size: 16px;
	background-color: #F9F9F9;
	color: #F00;
	font-weight: bold;
}
.mCenter {
	text-align: center;
	font-weight: bold;
}
</style>
</head>

<body>
<h1 class="mCenter"><strong>We thank the four reviewers 045A, 284F, 4BF9, and 0F1D for their review and valuable suggestions, and we have revised the text accordingly, and we are deeply grateful for your guidance.
</strong></h1>
<p><strong class="color">Response from Review 045A:</strong><br />
1. In terms of innovativeness, most of the  current image fusion algorithms perform fusion mainly based on features, while  only a few are semantic based. Our approach belongs to one of the few  semantic-based image fusion algorithms. We have introduced innovative semantic  prior knowledge in the field of image fusion, which, to our knowledge, is the  first time it has been systematically explored and presented in the literature.  Therefore, on this basis alone, our approach should not be categorised as  simply &quot;Minor originality&quot;.</p>
<p>2. In terms of performance, we have conducted a detailed comparison of our method with nine current leading methods, as shown in Table 1 of the paper, and our method performs well on the MSRS dataset, significantly outperforming the other nine methods, including even SeAFusion, which is widely regarded as the most superior method in the field of fusion, as mentioned by the reviewer. In order to fully compare the performance of our method with SeAFusion, we performed a generalised analysis comparison and selected the M3FD dataset. On this basis, we randomly selected 20 pairs of fusion results for mean calculation. The results show that our method also exhibits excellent performance on the M3FD dataset: it outperforms SeAFusion in terms of mutual information (MI), visual information fidelity (VIF), average gradient (AG), spatial frequency (SF), and quality of fusion (Qabf), which suggests that the fused images generated by our method are more similar to the original infrared images and visible images, with better image fidelity, presents richer details and variations, as well as obtains better fused image quality.<br />
</p>
<p class="mCenter"><span class="mCenter"><span class="mCenter"><span class="mCenter">Table 1:Comparative generalisation analysis of </span></span></span>SPGFusion<span class="mCenter"><span class="mCenter"><span class="mCenter"> with SeAFusion on M3FD dataset</span></span></span></p>
<div align="center">
  <table border="1" cellspacing="0" cellpadding="0" width="442">
    <tr>
      <td width="82" nowrap="nowrap"><strong><br />
        Method </strong></td>
      <td width="72" nowrap="nowrap"><p align="center"><strong>MI</strong></p></td>
      <td width="72" nowrap="nowrap"><p align="center"><strong>VIF</strong></p></td>
      <td width="72" nowrap="nowrap"><p align="center"><strong>AG</strong></p></td>
      <td width="72" nowrap="nowrap"><p align="center"><strong>Qabf</strong></p></td>
      <td width="72" nowrap="nowrap"><p align="center"><strong>SF</strong></p></td>
    </tr>
    <tr>
      <td width="82" nowrap="nowrap" valign="bottom"><p align="center">SeAFusion</p></td>
      <td width="72" nowrap="nowrap"><p align="center">2.8996</p></td>
      <td width="72" nowrap="nowrap"><p align="center">0.8261</p></td>
      <td width="72" nowrap="nowrap"><p align="center">4.4289</p></td>
      <td width="72" nowrap="nowrap"><p align="center">0.6711</p></td>
      <td width="72" nowrap="nowrap"><p align="center">13.7051</p></td>
    </tr>
    <tr>
      <td width="82" nowrap="nowrap" valign="bottom"><p align="center">Our</p></td>
      <td width="72" nowrap="nowrap"><p align="center">3.6612</p></td>
      <td width="72" nowrap="nowrap"><p align="center">0.864</p></td>
      <td width="72" nowrap="nowrap"><p align="center">4.5295</p></td>
      <td width="72" nowrap="nowrap"><p align="center">0.7044</p></td>
      <td width="72" nowrap="nowrap"><p align="center">13.8751</p></td>
    </tr>
  </table>
</div>
<p>Therefore, our method should not be oversimplified to &quot;Minor originality&quot; in terms of &quot;Novelty/Originality&quot;.<br />
</p>
<p>3. In order to verify the effect of adversarial loss and content loss separately, the results after removing only adversarial loss and only content loss are used as shown below: <br />
</p>
<p>Since retraining the network takes longer, we will mention this process in the article but will not show it here.</p>
<p>&nbsp;</p>
<p><span class="color"><strong class="color">Response from Review</strong> 284F:</span><br />
  1. The addition of the gradient penalty term has two main effects: </p>
<p>(i) it makes the penalty smaller for larger IR or visible gradient values, and larger for smaller gradient values, thus prompting the fused image to generate more edge and gradient information. </p>
<p>(ii) make the model input has a small change, the network weights will not produce too much change, as far as possible to reduce the generation of mode collapse problem.<br />
</p>
<p>2. The difference lies in 1. Wu et al. [17] used the transposed attention mechanism to compute the cost attention map, while we use two feature maps to multiply and then compute the attention map by softmax function.The similarity is that we compute the similarity between two features. advantages that Transformer has when dealing with differences between modal information.<br />
</p>
<p>&nbsp;</p>
<p><strong class="color"><span class="color">Response from Review</span></strong><span class="color"> 4BF9:</span></p>
<p class="mCenter">Table 2: SPGFusion efficiency analysis, which compares it to each of the nine current state-of-the-art methods.</p>
<div align="center">
  <table width="920" border="1" align="center" cellpadding="0" cellspacing="0">
    <tr>
      <td width="73" nowrap="nowrap"></td>
      <td width="89" nowrap="nowrap"><p align="center"><strong>FusionGan</strong></p></td>
      <td width="95" nowrap="nowrap"><p align="center"><strong>DenseFuse</strong></p></td>
      <td width="85" nowrap="nowrap"><p align="center"><strong>DIDFuse</strong></p></td>
      <td width="66" nowrap="nowrap"><p align="center"><strong>PMGI</strong></p></td>
      <td width="80" nowrap="nowrap"><p align="center"><strong>ICAFusion</strong></p></td>
      <td width="80" nowrap="nowrap"><p align="center"><strong>IFCNN</strong></p></td>
      <td width="76" nowrap="nowrap"><p align="center"><strong>GANMcc</strong></p></td>
      <td width="92" nowrap="nowrap"><p align="center"><strong>SeAFusion</strong></p></td>
      <td width="73" nowrap="nowrap"><p align="center"><strong>U2Fusion</strong></p></td>
      <td width="111" nowrap="nowrap"><p align="center"><strong>Our</strong></p></td>
    </tr>
    <tr>
      <td width="73" nowrap="nowrap"><p align="center">Size(MB)</p></td>
      <td width="89" nowrap="nowrap"><p align="center">0.074</p></td>
      <td width="95" nowrap="nowrap"><p align="center">0.075</p></td>
      <td width="85" nowrap="nowrap"><p align="center">0.261</p></td>
      <td width="66" nowrap="nowrap"><p align="center">0.042</p></td>
      <td width="80" nowrap="nowrap"><p align="center">2.471</p></td>
      <td width="80" nowrap="nowrap"><p align="center">0.084</p></td>
      <td width="76" nowrap="nowrap"><p align="center">1.864</p></td>
      <td width="92" nowrap="nowrap"><p align="center">0.167</p></td>
      <td width="73" nowrap="nowrap"><p align="center">0.659</p></td>
      <td width="111" nowrap="nowrap"><p align="center">3.727</p></td>
    </tr>
    <tr>
      <td width="73" nowrap="nowrap"><p align="center">Time(s)</p></td>
      <td width="89" nowrap="nowrap"><p align="center">0.122</p></td>
      <td width="95" nowrap="nowrap"><p align="center">3.435</p></td>
      <td width="85" nowrap="nowrap"><p align="center">0.053</p></td>
      <td width="66" nowrap="nowrap"><p align="center">0.058</p></td>
      <td width="80" nowrap="nowrap"><p align="center">9.398</p></td>
      <td width="80" nowrap="nowrap"><p align="center">0.021</p></td>
      <td width="76" nowrap="nowrap"><p align="center">0.338</p></td>
      <td width="92" nowrap="nowrap"><p align="center">0.655</p></td>
      <td width="73" nowrap="nowrap"><p align="center">0.057</p></td>
      <td width="111" nowrap="nowrap"><p align="center">4.708</p></td>
    </tr>
  </table>
</div>
<p>We analysed model size (Mb) and inference time (s) for all compared methods, including several state-of-the-art (SOTA) methods. Note that each model was performed on a single NVIDIA 3060Ti GPU, with the input scaled to 640 × 480. from the results in Table 1, our SPGFusion ranks highest in terms of model size, with a maximum parameter count of 3.727 M. Meanwhile, the inference time ranks second to the bottom, a phenomenon attributed to our adopted a semantic feature-aware approach. Therefore, how to achieve a better balance between performance and efficiency has become an important direction for our future research.<br />
</p>
<p>&nbsp;</p>
<p><strong class="color"><span class="color">Response from Review</span></strong><span class="color"> 0F1D :</span><br />
  1. Modifications have been made to Figures 1 and 2 and the cited literature is labelled in Table 1.</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
</body>
</html>
